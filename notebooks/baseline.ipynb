{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.functions import create_model, plot_data, check_gpus, create_train_val_datasets, load_stations_from_path, \\\n",
    "    create_test_datasets, plot_keras_history, get_features_and_targets\n",
    "from src.utils import now_formatted, setup_logger, format_with_border, measure_execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "mlflow_port = os.getenv('MLFLOW_PORT')\n",
    "mlflow_uri = f'http://localhost:{mlflow_port}'\n",
    "mlflow_experiment_name = f'Benchmark'\n",
    "train_path = os.getenv('LS_LABELED_TRAIN_DATA_PATH')\n",
    "test_path = os.getenv('LS_LABELED_TEST_DATA_PATH')\n",
    "log_file_path = '/tmp/benchmark.log'\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_uri)\n",
    "mlflow.set_experiment(mlflow_experiment_name)\n",
    "mlflow.tensorflow.autolog()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 20\n",
    "TARGET_START_INDEX = SEQUENCE_LENGTH - 1\n",
    "FEATURE_COLUMNS = ['HS', 'day_sin', 'day_cos', 'month_sin', 'month_cos']\n",
    "TARGET_COLUMN = 'no_snow'\n",
    "DATE_COLUMN = 'measure_date'\n",
    "SPLIT_PERCENTAGE = 0.8\n",
    "DATASET_BATCH_SIZE = 64\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ARCHITECTURE = \"128(l)-64-8(d)-1\"\n",
    "MODEL_INPUT_SHAPE = (SEQUENCE_LENGTH, len(FEATURE_COLUMNS))\n",
    "MODEL_DROPOUT_RATE = 0.5\n",
    "MODEL_OPTIMIZER = 'adam'\n",
    "MODEL_METRICS = ['accuracy']\n",
    "MODEL_LOSS = 'binary_crossentropy'\n",
    "MODEL_BATCH_SIZE = 64\n",
    "MODEL_EPOCHS = 20\n",
    "\n",
    "def log_parameters(logging, mlflow):\n",
    "    global_vars = globals()\n",
    "    for var_name, value in global_vars.items():\n",
    "        if var_name.isupper():\n",
    "            logging.info(f'{var_name}: {value}')\n",
    "            mlflow.log_param(f'benchmark_{var_name.lower()}', value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_stations = load_stations_from_path(train_path)\n",
    "testing_stations = {station.iloc[0]['station_code']: station for station in load_stations_from_path(test_path)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=now_formatted()):\n",
    "    logging, tmp_log_file = setup_logger(log_file_path=log_file_path)\n",
    "    logging.info(format_with_border('Starting experiment'))\n",
    "    \n",
    "    log_parameters(logging, mlflow)\n",
    "    logging.info(format_with_border('Preparing Training Data'))\n",
    "    train_dataset, val_dataset, mean, std, num_train_samples, num_val_samples, _ = create_train_val_datasets(\n",
    "        training_stations, SPLIT_PERCENTAGE, FEATURE_COLUMNS, TARGET_COLUMN, SEQUENCE_LENGTH, TARGET_START_INDEX, DATASET_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Training samples: {num_train_samples}\")\n",
    "    logging.info(f\"Validation samples: {num_val_samples}\")\n",
    "    mlflow.log_param('benchmark_training_samples', num_train_samples)\n",
    "    mlflow.log_param('benchmark_validation_samples', num_val_samples)\n",
    "\n",
    "    model = create_model(MODEL_ARCHITECTURE, MODEL_INPUT_SHAPE, logging=None, dropout_rate=0.5, summary=False)\n",
    "    model.compile(\n",
    "        optimizer=MODEL_OPTIMIZER,\n",
    "        metrics=MODEL_METRICS,\n",
    "        loss=MODEL_LOSS\n",
    "    )\n",
    "\n",
    "    logging.info(format_with_border('Fitting Model'))\n",
    "    @measure_execution_time\n",
    "    def fit_model():\n",
    "        return model.fit(\n",
    "            train_dataset,\n",
    "            epochs=MODEL_EPOCHS,\n",
    "            batch_size=MODEL_BATCH_SIZE,\n",
    "            validation_data=val_dataset\n",
    "        )\n",
    "    history, elapsed_fitting_time = fit_model()\n",
    "    logging.info(f'Model fitting completed in {elapsed_fitting_time}')\n",
    "    mlflow.log_param('benchmark_model_fitting_time', elapsed_fitting_time)\n",
    "\n",
    "    logging.info(format_with_border('Evaluating Model on Test Data'))\n",
    "    test_datasets = create_test_datasets(\n",
    "        testing_stations.values(), FEATURE_COLUMNS, TARGET_COLUMN, SEQUENCE_LENGTH, TARGET_START_INDEX, DATASET_BATCH_SIZE, mean, std\n",
    "    )\n",
    "    # TODO: Convert to shared function\n",
    "    all_evaluation_results = np.empty((0, 2), float)\n",
    "    for j, dataset in enumerate(test_datasets):\n",
    "        evaluation_results = model.evaluate(dataset, verbose=0)\n",
    "        station_name = list(testing_stations.keys())[j]\n",
    "        test_df = list(testing_stations.values())[j]\n",
    "        logging.info(\n",
    "            f'Station: {station_name}, Samples: {len(test_df)}, Loss: {evaluation_results[0]:.2f}, Accuracy: {evaluation_results[1]:.2f}'\n",
    "        )\n",
    "        all_evaluation_results = np.append(all_evaluation_results, [evaluation_results], axis=0)\n",
    "\n",
    "    mlflow.log_metric('test_avg_loss', np.mean(all_evaluation_results[:, 0]))\n",
    "    mlflow.log_metric('test_avg_accuracy', np.mean(all_evaluation_results[:, 1]))\n",
    "\n",
    "    # Plotting\n",
    "    predictions = [model.predict(td, verbose=0).reshape((-1,)) > 0.5 for td in test_datasets]\n",
    "    fig = plot_data(\n",
    "        [test_station[TARGET_START_INDEX:] for test_station in testing_stations.values()],\n",
    "        predictions=predictions,\n",
    "        show=False\n",
    "    )\n",
    "    mlflow.log_figure(fig, 'prediction_results.png')\n",
    "    mlflow.log_artifact(tmp_log_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history_plot = plot_keras_history(history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Smote implementation TEST, if not working sohuld be removed from here\n",
    "from matplotlib import pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "features, targets, mean, std = get_features_and_targets(training_stations[0], len(training_stations[0]), ['HS'], ['no_snow'])\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(20, 5))\n",
    "# plt.plot(range(len(features.flatten())), features.flatten())\n",
    "# plt.show()\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "x_resample, y_resample = smote.fit_resample(features, targets)\n",
    "len(targets), len(y_resample), np.unique(targets, return_counts=True), np.unique(y_resample, return_counts=True)\n",
    "training_stations[0].head()\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(range(len(features.flatten())), features.flatten())\n",
    "plt.title('Original')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(range(len(x_resample.flatten())), x_resample.flatten())\n",
    "plt.title('Smote')\n",
    "plt.show()\n",
    "\n",
    "for ts in training_stations:\n",
    "    ts['no_snow'] = ts['no_snow'].astype(int)\n",
    "    \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "weights = []\n",
    "\n",
    "for i, ts in enumerate(training_stations):\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(training_stations[i]['no_snow']), y=training_stations[i]['no_snow'])\n",
    "    weights.append(class_weights)\n",
    "    \n",
    "class_weights = dict(enumerate(np.array(weights).mean(axis=0)))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
