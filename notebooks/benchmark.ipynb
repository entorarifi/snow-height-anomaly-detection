{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:02.637766965Z",
     "start_time": "2023-10-30T20:09:02.634843976Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n405d3rg96\r\n"
     ]
    }
   ],
   "source": [
    "! hostname"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:03.993815867Z",
     "start_time": "2023-10-30T20:09:02.922466822Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:04.010223194Z",
     "start_time": "2023-10-30T20:09:03.989467734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:04.011092999Z",
     "start_time": "2023-10-30T20:09:03.991824763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:11.255083257Z",
     "start_time": "2023-10-30T20:09:11.134931417Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/merged.csv')\n",
    "# df = pd.read_csv('../data/merged_labeled_daily.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:12.021177090Z",
     "start_time": "2023-10-30T20:09:11.956732465Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=df.columns[:2])\n",
    "df['measure_date'] = pd.to_datetime(df['measure_date'])\n",
    "df['year'] = df['measure_date'].dt.year\n",
    "df['month'] = df['measure_date'].dt.month\n",
    "df['day'] = df['measure_date'].dt.day\n",
    "df['weekday'] = df['measure_date'].dt.weekday\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:13.093330797Z",
     "start_time": "2023-10-30T20:09:13.078527008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  station_code measure_date    HS  no_snow  anomaly  year  month  day  \\\n0         AMD2   1997-10-14  12.0    False    False  1997     10   14   \n1         AMD2   1997-10-15  31.0    False    False  1997     10   15   \n2         AMD2   1997-10-16   9.0    False    False  1997     10   16   \n3         AMD2   1997-10-17  16.0    False    False  1997     10   17   \n4         AMD2   1997-10-18  -4.0     True    False  1997     10   18   \n\n   weekday  month_sin  month_cos   day_sin   day_cos  \n0        1  -0.866025        0.5  0.299363 -0.954139  \n1        2  -0.866025        0.5  0.101168 -0.994869  \n2        3  -0.866025        0.5 -0.101168 -0.994869  \n3        4  -0.866025        0.5 -0.299363 -0.954139  \n4        5  -0.866025        0.5 -0.485302 -0.874347  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>station_code</th>\n      <th>measure_date</th>\n      <th>HS</th>\n      <th>no_snow</th>\n      <th>anomaly</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>weekday</th>\n      <th>month_sin</th>\n      <th>month_cos</th>\n      <th>day_sin</th>\n      <th>day_cos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AMD2</td>\n      <td>1997-10-14</td>\n      <td>12.0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1997</td>\n      <td>10</td>\n      <td>14</td>\n      <td>1</td>\n      <td>-0.866025</td>\n      <td>0.5</td>\n      <td>0.299363</td>\n      <td>-0.954139</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AMD2</td>\n      <td>1997-10-15</td>\n      <td>31.0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1997</td>\n      <td>10</td>\n      <td>15</td>\n      <td>2</td>\n      <td>-0.866025</td>\n      <td>0.5</td>\n      <td>0.101168</td>\n      <td>-0.994869</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AMD2</td>\n      <td>1997-10-16</td>\n      <td>9.0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1997</td>\n      <td>10</td>\n      <td>16</td>\n      <td>3</td>\n      <td>-0.866025</td>\n      <td>0.5</td>\n      <td>-0.101168</td>\n      <td>-0.994869</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AMD2</td>\n      <td>1997-10-17</td>\n      <td>16.0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1997</td>\n      <td>10</td>\n      <td>17</td>\n      <td>4</td>\n      <td>-0.866025</td>\n      <td>0.5</td>\n      <td>-0.299363</td>\n      <td>-0.954139</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AMD2</td>\n      <td>1997-10-18</td>\n      <td>-4.0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>1997</td>\n      <td>10</td>\n      <td>18</td>\n      <td>5</td>\n      <td>-0.866025</td>\n      <td>0.5</td>\n      <td>-0.485302</td>\n      <td>-0.874347</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 88601\n",
      "Validation length: 37972\n",
      "Test length: 42749\n",
      "169322 169322 True\n"
     ]
    }
   ],
   "source": [
    "num_train_stations = 15\n",
    "stations = df['station_code'].unique()\n",
    "\n",
    "train_val_end_index = df[df['station_code'] == stations[num_train_stations - 1]].index[-1] + 1\n",
    "\n",
    "num_train_samples = int(0.7 * train_val_end_index)\n",
    "num_val_samples = train_val_end_index - num_train_samples\n",
    "num_test_samples = len(df) - train_val_end_index\n",
    "\n",
    "print(\"Train length: %s\" % num_train_samples)\n",
    "print(\"Validation length: %s\" % num_val_samples)\n",
    "print(\"Test length: %s\" % num_test_samples)\n",
    "\n",
    "total = num_train_samples + num_val_samples + num_test_samples\n",
    "print(total, len(df), total == len(df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:13.526485639Z",
     "start_time": "2023-10-30T20:09:13.440877194Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "features = df[['HS', 'day_sin', 'day_cos', 'month_sin', 'month_cos']].values\n",
    "target = df[['no_snow']].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:15.423804736Z",
     "start_time": "2023-10-30T20:09:15.413505292Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.65719526  0.41694945 -1.33643084 -1.22485271  0.69111721]\n",
      " [-0.46035473  0.13911521 -1.39456923 -1.22485271  0.69111721]\n",
      " [-0.68827535 -0.14452516 -1.39456923 -1.22485271  0.69111721]\n",
      " ...\n",
      " [ 0.2783153   1.10582347 -0.84821005  1.23528619  0.69111721]\n",
      " [ 0.23791118  0.91040464 -1.05754238  1.23528619  0.69111721]\n",
      " [ 0.26277525  0.67760299 -1.22253424  1.23528619  0.69111721]]\n"
     ]
    }
   ],
   "source": [
    "mean = features[:num_train_samples].mean(axis=0)\n",
    "features -= mean\n",
    "std = features[:num_train_samples].std(axis=0)\n",
    "features /= std\n",
    "print(features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:15.727980008Z",
     "start_time": "2023-10-30T20:09:15.721543895Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]\n",
      " [False]\n",
      " [False]]\n",
      "[[-0.65719526  0.41694945 -1.33643084 -1.22485271  0.69111721]\n",
      " [-0.46035473  0.13911521 -1.39456923 -1.22485271  0.69111721]\n",
      " [-0.68827535 -0.14452516 -1.39456923 -1.22485271  0.69111721]]\n"
     ]
    }
   ],
   "source": [
    "print(target[:3])\n",
    "print(features[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:19.071983040Z",
     "start_time": "2023-10-30T20:09:19.060499135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:20.132241099Z",
     "start_time": "2023-10-30T20:09:20.127915201Z"
    }
   },
   "outputs": [],
   "source": [
    "def sequence_debugging():\n",
    "    int_sequence = np.arange(30)\n",
    "\n",
    "    train_end_index = int(0.5 * len(int_sequence))\n",
    "    val_end_index = int(0.75 * len(int_sequence))\n",
    "\n",
    "    sequence_length = 3\n",
    "    target_start_idx = sequence_length - 1\n",
    "\n",
    "    train_dataset = keras.utils.timeseries_dataset_from_array(\n",
    "        data=int_sequence,\n",
    "        targets=int_sequence[target_start_idx:],\n",
    "        sequence_length=sequence_length,\n",
    "        start_index=0,\n",
    "        end_index=train_end_index\n",
    "    )\n",
    "\n",
    "    val_dataset = keras.utils.timeseries_dataset_from_array(\n",
    "        data=int_sequence,\n",
    "        targets=int_sequence[target_start_idx:],\n",
    "        sequence_length=sequence_length,\n",
    "        start_index=train_end_index,\n",
    "        end_index=val_end_index\n",
    "    )\n",
    "\n",
    "    test_dataset = keras.utils.timeseries_dataset_from_array(\n",
    "        data=int_sequence,\n",
    "        targets=int_sequence[target_start_idx:],\n",
    "        sequence_length=sequence_length,\n",
    "        start_index=val_end_index\n",
    "    )\n",
    "\n",
    "    print(\"Train:\")\n",
    "    for inputs, targets in train_dataset:\n",
    "        for i in range(inputs.shape[0]):\n",
    "            print([int(x) for x in inputs[i]], int(targets[i]))\n",
    "\n",
    "            \n",
    "    print(\"\\nVal:\")\n",
    "    for inputs, targets in val_dataset:\n",
    "        for i in range(inputs.shape[0]):\n",
    "            print([int(x) for x in inputs[i]], int(targets[i]))\n",
    "\n",
    "    print(\"\\nTest:\")\n",
    "    for inputs, targets in test_dataset:\n",
    "        for i in range(inputs.shape[0]):\n",
    "            print([int(x) for x in inputs[i]], int(targets[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:25.425885020Z",
     "start_time": "2023-10-30T20:09:24.890718365Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "sequence_length = 30\n",
    "sequence_stride = 1\n",
    "sampling_rate = 1\n",
    "sequence_batch_size = 128\n",
    "# batch_size = None\n",
    "shuffle_train_val = True\n",
    "shuffle_test = False\n",
    "start_index = 0\n",
    "\n",
    "target_start_idx = sequence_length - 1\n",
    "\n",
    "def create_dataset(start_index, end_index, shuffle=False):\n",
    "    return keras.utils.timeseries_dataset_from_array(\n",
    "        data=features,\n",
    "        targets=target[target_start_idx:],\n",
    "        sequence_length=sequence_length,\n",
    "        sequence_stride=sequence_stride,\n",
    "        sampling_rate=1,\n",
    "        batch_size=sequence_batch_size,\n",
    "        shuffle=shuffle,\n",
    "        start_index=start_index,\n",
    "        end_index=end_index\n",
    "    )\n",
    "\n",
    "train_dataset = create_dataset(start_index=0, end_index=num_train_samples, shuffle=shuffle_train_val)\n",
    "val_dataset = create_dataset(start_index=num_train_samples, end_index=num_train_samples + num_val_samples, shuffle=shuffle_train_val)\n",
    "test_dataset = create_dataset(start_index=num_train_samples + num_val_samples, end_index=(len(df) - 1), shuffle=shuffle_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class MonteCarloDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs, training=None):\n",
    "        return super().call(inputs, training=True)\n",
    "    \n",
    "def create_network(architecture, input_shape, dropout_rate=0.5, summary=True):\n",
    "    arch_split = architecture.split('-')\n",
    "    dense = True\n",
    "    bidirectional = False\n",
    "    layers = []\n",
    "\n",
    "    digits_pattern = re.compile(r\"\\d+\")\n",
    "    \n",
    "    if 'l' in arch_split[0]:\n",
    "        rnn_layer = 'LSTM'\n",
    "    elif 'g' in arch_split[0]:\n",
    "        rnn_layer = 'GRU'\n",
    "    else:\n",
    "        raise Exception('rnn_layers must be one of [LSTM, GRU]')\n",
    "\n",
    "    if 'b' in arch_split[0]:\n",
    "        bidirectional = True\n",
    "        \n",
    "    for i, layer in enumerate(reversed(arch_split)):\n",
    "        no_units = int(digits_pattern.findall(layer)[0])\n",
    "\n",
    "        if dense:\n",
    "            activation = 'sigmoid' if i == 0 else 'relu'\n",
    "            layers.append(keras.layers.Dense(no_units, activation=activation))\n",
    "        else:\n",
    "            args = {\n",
    "                'units': no_units,\n",
    "            }\n",
    "\n",
    "            if '(d)' not in arch_split[-i]:\n",
    "                args['return_sequences'] = True\n",
    "\n",
    "            if i == len(arch_split) - 1:\n",
    "                args['input_shape'] = input_shape\n",
    "                \n",
    "            current_layer = keras.layers.LSTM(**args) if rnn_layer == 'LSTM' else keras.layers.GRU(**args)\n",
    "            \n",
    "            if bidirectional:\n",
    "                current_layer = keras.layers.Bidirectional(current_layer)\n",
    "            \n",
    "            layers.extend([\n",
    "                MonteCarloDropout(dropout_rate),\n",
    "                current_layer\n",
    "            ])\n",
    "            \n",
    "        if '(d)' in layer:\n",
    "            dense = False\n",
    "            \n",
    "    layers.reverse()\n",
    "    \n",
    "    if summary:\n",
    "        for layer in layers:\n",
    "            layer_type = str(type(layer))\n",
    "\n",
    "            if 'Dense' in str(type(layer)):\n",
    "                print(type(layer), layer.units, layer.activation)\n",
    "            elif 'Bidirectional' in str(type(layer)):\n",
    "                print(type(layer), layer.layer, layer.layer.units, layer.layer.activation, layer.layer.return_sequences)\n",
    "            elif 'LSTM' in str(type(layer)):\n",
    "                print(type(layer), layer.units, layer.activation, layer.return_sequences)\n",
    "            elif 'GRU' in str(type(layer)):\n",
    "                print(type(layer), layer.units, layer.activation, layer.return_sequences)\n",
    "            elif 'Dropout' in layer_type:\n",
    "                print(type(layer), layer.rate)\n",
    "\n",
    "    return layers\n",
    "\n",
    "# arch = \"128(gb)-64-8(d)-1\"\n",
    "# input_shape = (sequence_length, features.shape[-1])\n",
    "# network = create_network(arch, input_shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:27.865189432Z",
     "start_time": "2023-10-30T20:09:27.863772285Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.layers.rnn.lstm.LSTM'> 128 <function tanh at 0x7fbfb16e9040> True\n",
      "<class '__main__.MonteCarloDropout'> 0.5\n",
      "<class 'keras.layers.rnn.lstm.LSTM'> 64 <function tanh at 0x7fbfb16e9040> False\n",
      "<class '__main__.MonteCarloDropout'> 0.5\n",
      "<class 'keras.layers.core.dense.Dense'> 8 <function relu at 0x7fbfb16e4ca0>\n",
      "<class 'keras.layers.core.dense.Dense'> 1 <function sigmoid at 0x7fbfb16e91f0>\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [22], line 18\u001B[0m\n\u001B[1;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m keras\u001B[38;5;241m.\u001B[39mSequential(network)\n\u001B[1;32m     12\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[1;32m     13\u001B[0m     optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     14\u001B[0m     metrics\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     15\u001B[0m     loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     16\u001B[0m )\n\u001B[0;32m---> 18\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# callbacks=[\u001B[39;49;00m\n\u001B[1;32m     24\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# keras.callbacks.ModelCheckpoint(checkpoint_name, save_best_only=True),\u001B[39;49;00m\n\u001B[1;32m     25\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1) \u001B[39;49;00m\n\u001B[1;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# ]\u001B[39;49;00m\n\u001B[1;32m     27\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py:1409\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1402\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1403\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   1404\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   1405\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[1;32m   1406\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   1407\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m   1408\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1409\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1410\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1411\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py:980\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    976\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# Fall through to cond-based initialization.\u001B[39;00m\n\u001B[1;32m    977\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    978\u001B[0m     \u001B[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001B[39;00m\n\u001B[1;32m    979\u001B[0m     \u001B[38;5;66;03m# stateless function.\u001B[39;00m\n\u001B[0;32m--> 980\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stateless_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    981\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    982\u001B[0m   _, _, filtered_flat_args \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    983\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn\u001B[38;5;241m.\u001B[39m_function_spec\u001B[38;5;241m.\u001B[39mcanonicalize_function_inputs(  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    984\u001B[0m           \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds))\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py:2453\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2450\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m   2451\u001B[0m   (graph_function,\n\u001B[1;32m   2452\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m-> 2453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2454\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py:1860\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1856\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1857\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1858\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1859\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1860\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1861\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1862\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1863\u001B[0m     args,\n\u001B[1;32m   1864\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1865\u001B[0m     executing_eagerly)\n\u001B[1;32m   1866\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py:497\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    495\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    496\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 497\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    503\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    504\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    505\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    506\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    509\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    510\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "arch = \"128(l)-64-8(d)-1\"\n",
    "input_shape = (sequence_length, features.shape[-1])\n",
    "epochs = 10\n",
    "model_batch_size = 64\n",
    "checkpoint_name = 'snow_height_anomaly_multi_vars_gru.keras'\n",
    "current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_name = f\"{current_time}_arch-{arch}_epochs-{epochs}_batch-{model_batch_size}_seq-{sequence_length}_num-train-stations-{num_train_stations}\"\n",
    "log_dir = f\"./logs/{model_name}\"\n",
    "\n",
    "network = create_network(arch, input_shape)\n",
    "model = keras.Sequential(network)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    metrics='accuracy',\n",
    "    loss='binary_crossentropy'\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    batch_size=model_batch_size,\n",
    "    validation_data=val_dataset,\n",
    "    # callbacks=[\n",
    "        # keras.callbacks.ModelCheckpoint(checkpoint_name, save_best_only=True),\n",
    "        # keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1) \n",
    "    # ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:43.984215359Z",
     "start_time": "2023-10-30T20:09:36.823235609Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T20:09:58.169460428Z",
     "start_time": "2023-10-30T20:09:58.153036294Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [23], line 25\u001B[0m\n\u001B[1;32m     22\u001B[0m     plt\u001B[38;5;241m.\u001B[39mtight_layout()\n\u001B[1;32m     23\u001B[0m     plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[0;32m---> 25\u001B[0m plot_keras_history(\u001B[43mhistory\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_keras_history(history):\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_keras_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 [==============================] - 5s 15ms/step - loss: 0.2361 - accuracy: 0.9214\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.23606866598129272, 0.9214401245117188]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T19:30:31.838236444Z",
     "start_time": "2023-10-29T19:30:26.684845060Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T20:10:04.914806825Z",
     "start_time": "2023-10-30T20:10:04.913469414Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_data(dfs, y='HS', target='no_snow', predictions=None):\n",
    "    if not isinstance(dfs, list):\n",
    "        dfs = [dfs]\n",
    "\n",
    "    if not isinstance(predictions, list):\n",
    "        predictions = [predictions] if predictions else []\n",
    "\n",
    "    if not isinstance(y, list):\n",
    "        y = [y] * len(dfs)\n",
    "\n",
    "    if not isinstance(target, list):\n",
    "        target = [target] * len(dfs)\n",
    "\n",
    "    if len(predictions):\n",
    "        rows = len(dfs) * 2\n",
    "    else:\n",
    "        rows = len(dfs)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, 1, figsize=(20, 5 * rows))\n",
    "\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        plot_df = df.copy()\n",
    "        plot_df.index = pd.to_datetime(plot_df['measure_date'])\n",
    "        station_code = plot_df['station_code'].iloc[0]\n",
    "\n",
    "        # Plot original data\n",
    "        ax = axes[2 * i] if len(predictions) else axes[i]\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel(y[i])\n",
    "        ax.set_title(f'Station: {station_code} | Original Data with Anomalies Highlighted')\n",
    "        ax.plot(plot_df.index, plot_df[y[i]], label='Data', marker='o', linestyle='-', ms=4)\n",
    "        ax.scatter(plot_df[plot_df[target[i]]].index, plot_df[plot_df[target[i]]][y[i]], color='red', label='Anomalies', zorder=5, s=20)\n",
    "        ax.legend()\n",
    "\n",
    "        # Plot predictions\n",
    "        if len(predictions):\n",
    "            ax = axes[2 * i + 1]\n",
    "            ax.set_xlabel('Date')\n",
    "            ax.set_ylabel(y[i])\n",
    "            ax.set_title(f'Station: {station_code} | Predicted Data with Anomalies Highlighted')\n",
    "            ax.plot(plot_df.index, plot_df[y[i]], label='Data', marker='o', linestyle='-', ms=4)\n",
    "            ax.scatter(plot_df[predictions[i]].index, plot_df[predictions[i]][y[i]], color='green', label='Predicted Anomalies', zorder=5, s=20)\n",
    "            ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "test_stations = df[train_val_end_index:]['station_code'].unique()\n",
    "test_stations = [(df[df['station_code'] == station].index.values[[0, -1]], df[df['station_code'] == station][sequence_length:]) for station in test_stations]\n",
    "predictions = [model.predict(create_dataset(station[0][0], station[0][1])).reshape((-1,)) > 0.5 for station in test_stations]\n",
    "# \n",
    "plot_data(\n",
    "    [station[1] for station in test_stations],\n",
    "    predictions=predictions\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T19:32:44.610850564Z",
     "start_time": "2023-10-29T19:32:44.478171807Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 6ms/step\n",
      "75/75 [==============================] - 0s 5ms/step\n",
      "75/75 [==============================] - 0s 6ms/step\n",
      "75/75 [==============================] - 0s 5ms/step\n",
      "75/75 [==============================] - 0s 5ms/step\n",
      "75/75 [==============================] - 0s 5ms/step\n",
      "75/75 [==============================] - 0s 6ms/step\n",
      "75/75 [==============================] - 0s 6ms/step\n",
      "75/75 [==============================] - 1s 6ms/step\n",
      "75/75 [==============================] - 0s 6ms/step\n",
      "75/75 [==============================] - 0s 6ms/step\n",
      "75/75 [==============================] - 0s 6ms/step\n",
      "75/75 [==============================] - 1s 6ms/step\n",
      "75/75 [==============================] - 1s 7ms/step\n",
      "75/75 [==============================] - 0s 6ms/step\n",
      "75/75 [==============================] - 1s 6ms/step\n",
      "75/75 [==============================] - 1s 6ms/step\n",
      "75/75 [==============================] - 1s 6ms/step\n",
      "75/75 [==============================] - 1s 6ms/step\n",
      "75/75 [==============================] - 1s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 7ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "41/41 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "66/66 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 7ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 7ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 7ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 1s 7ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "67/67 [==============================] - 0s 6ms/step\n",
      "87/87 [==============================] - 1s 6ms/step\n",
      "87/87 [==============================] - 1s 6ms/step\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "87/87 [==============================] - 1s 8ms/step\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "87/87 [==============================] - 1s 6ms/step\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "87/87 [==============================] - 1s 8ms/step\n",
      "87/87 [==============================] - 1s 8ms/step\n",
      "87/87 [==============================] - 1s 8ms/step\n",
      "87/87 [==============================] - 1s 9ms/step\n",
      "87/87 [==============================] - 1s 8ms/step\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "87/87 [==============================] - 1s 8ms/step\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "87/87 [==============================] - 1s 7ms/step\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.1313 - accuracy: 0.9533\n",
      "41/41 [==============================] - 0s 8ms/step - loss: 0.1361 - accuracy: 0.9527\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 0.3559 - accuracy: 0.8762\n",
      "67/67 [==============================] - 1s 9ms/step - loss: 0.3048 - accuracy: 0.9086\n",
      "87/87 [==============================] - 1s 7ms/step - loss: 0.2320 - accuracy: 0.9238\n",
      "[[ 1.         -0.57381575]\n",
      " [-0.57381575  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def predict_with_uncertainty(model, x, n_iter=100):\n",
    "    preds = np.array([model.predict(x) for _ in range(n_iter)])\n",
    "    uncertainty = np.std(preds, axis=0)\n",
    "    scaled_uncertainty = (uncertainty - uncertainty.min()) / (uncertainty.max() - uncertainty.min())\n",
    "    return scaled_uncertainty.mean()\n",
    "\n",
    "uncertainties = [predict_with_uncertainty(model, create_dataset(station[0][0], station[0][1]), n_iter=20) for station in test_stations]\n",
    "accuracy = [model.evaluate(create_dataset(station[0][0], station[0][1]))[1] for station in test_stations]\n",
    "\n",
    "print(np.corrcoef(uncertainties, accuracy))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T20:25:36.141583254Z",
     "start_time": "2023-10-29T20:24:40.787719324Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04935129, 0.049654353, 0.08818548, 0.032795604, 0.041576687]\n",
      "[0.9533082246780396, 0.9526672959327698, 0.8761699199676514, 0.9086413979530334, 0.9238069653511047]\n",
      "-0.5738157518119447\n"
     ]
    }
   ],
   "source": [
    "print(uncertainties)\n",
    "print(accuracy)\n",
    "print(np.corrcoef(uncertainties, accuracy)[0, 1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T20:26:07.799849547Z",
     "start_time": "2023-10-29T20:26:07.755504566Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Pick random sample out of dataset\n",
    "\n",
    "\n",
    "# 3. Run a model on it\n",
    "\n",
    "# 4. Predict data on all other models\n",
    "\n",
    "# 5. Assign uncertainty scores to all other models\n",
    "\n",
    "# 6. Pick most uncertain sample\n",
    "\n",
    "# 7. Repeat\n",
    "\n",
    "# 4. Assign uncertainty scores to all other models\n",
    "\n",
    "#"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
