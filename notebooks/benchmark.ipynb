{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import seaborn as sns\n",
    "from keras.src.metrics import Recall, Precision, F1Score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.functions import create_model, plot_data, check_gpus, create_train_val_datasets, load_stations_from_path, \\\n",
    "    create_test_datasets, plot_keras_history\n",
    "from src.utils import now_formatted, setup_logger, format_with_border, measure_execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "mlflow_port = os.getenv('MLFLOW_PORT')\n",
    "mlflow_uri = f'http://localhost:{mlflow_port}'\n",
    "mlflow_experiment_name = f'Benchmark'\n",
    "train_path = '../data/labeled_benchmark/train'\n",
    "test_path = '../data/labeled_benchmark/test'\n",
    "log_file_path = '/tmp/benchmark.log'\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_uri)\n",
    "mlflow.set_experiment(mlflow_experiment_name)\n",
    "mlflow.tensorflow.autolog()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 90\n",
    "TARGET_START_INDEX = SEQUENCE_LENGTH - 1\n",
    "FEATURE_COLUMNS = [\n",
    "    'HS',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'TSS_30MIN_MEAN',\n",
    "    'RSWR_30MIN_MEAN',\n",
    "    'TA_30MIN_MEAN',\n",
    "    'VW_30MIN_MEAN'\n",
    "]\n",
    "TARGET_COLUMN = 'no_snow'\n",
    "DATE_COLUMN = 'measure_date'\n",
    "SPLIT_PERCENTAGE = 0.8\n",
    "DATASET_BATCH_SIZE = 64\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ARCHITECTURE = \"128(l)-64-8(d)-1\"\n",
    "MODEL_INPUT_SHAPE = (SEQUENCE_LENGTH, len(FEATURE_COLUMNS))\n",
    "MODEL_DROPOUT_RATE = 0.5\n",
    "MODEL_OPTIMIZER = 'adam'\n",
    "MODEL_METRICS = ['accuracy', Recall(), Precision()],\n",
    "\n",
    "MODEL_LOSS = 'binary_crossentropy'\n",
    "MODEL_BATCH_SIZE = 64\n",
    "MODEL_EPOCHS = 15\n",
    "\n",
    "EXPERIMENT_NAME = 'all_variables_truncated_if_one_missing'\n",
    "\n",
    "def log_parameters(logging, mlflow):\n",
    "    global_vars = globals()\n",
    "    for var_name, value in global_vars.items():\n",
    "        if var_name.isupper():\n",
    "            logging.info(f'{var_name}: {value}')\n",
    "            mlflow.log_param(f'benchmark_{var_name.lower()}', value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_confusion_matrix():\n",
    "    y_true = [ts.loc[TARGET_START_INDEX:, 'no_snow'].values for ts in testing_stations.values()]\n",
    "    y_pred = [model.predict(td, verbose=0).reshape((-1,)) > 0.5 for td in test_datasets]\n",
    "    n_matrices = len(y_true)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_matrices + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 5))\n",
    "    fig.tight_layout(pad=5.0)\n",
    "\n",
    "    for i in range(n_matrices):\n",
    "        ax = axes[i // n_cols, i % n_cols]\n",
    "        cm = confusion_matrix(y_true[i], y_pred[i])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=\"crest\", ax=ax)\n",
    "        ax.set_title(list(testing_stations.keys())[i])\n",
    "        ax.set_ylabel('Actual')\n",
    "        ax.set_xlabel('Predicted')\n",
    "\n",
    "    for j in range(i + 1, n_rows * n_cols):\n",
    "        axes[j // n_cols, j % n_cols].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_stations = load_stations_from_path(train_path)\n",
    "testing_stations = {station.iloc[0]['station_code']: station for station in load_stations_from_path(test_path)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=f\"{now_formatted()}_{EXPERIMENT_NAME}\"):\n",
    "    logging, tmp_log_file = setup_logger(log_file_path=log_file_path)\n",
    "    logging.info(format_with_border('Starting experiment'))\n",
    "    \n",
    "    log_parameters(logging, mlflow)\n",
    "    logging.info(format_with_border('Preparing Training Data'))\n",
    "    train_dataset, val_dataset, mean, std, num_train_samples, num_val_samples, _ = create_train_val_datasets(\n",
    "        training_stations, SPLIT_PERCENTAGE, FEATURE_COLUMNS, TARGET_COLUMN, SEQUENCE_LENGTH, TARGET_START_INDEX, DATASET_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Training samples: {num_train_samples}\")\n",
    "    logging.info(f\"Validation samples: {num_val_samples}\")\n",
    "    mlflow.log_param('benchmark_training_samples', num_train_samples)\n",
    "    mlflow.log_param('benchmark_validation_samples', num_val_samples)\n",
    "\n",
    "    model = create_model(MODEL_ARCHITECTURE, MODEL_INPUT_SHAPE, logging=None, dropout_rate=MODEL_DROPOUT_RATE, summary=False, dropout_layer='dropout')\n",
    "    model.compile(\n",
    "        optimizer=MODEL_OPTIMIZER,\n",
    "        metrics=MODEL_METRICS,\n",
    "        loss=MODEL_LOSS\n",
    "    )\n",
    "    \n",
    "    logging.info(format_with_border('Fitting Model'))\n",
    "    @measure_execution_time\n",
    "    def fit_model():\n",
    "        return model.fit(\n",
    "            train_dataset,\n",
    "            epochs=MODEL_EPOCHS,\n",
    "            batch_size=MODEL_BATCH_SIZE,\n",
    "            validation_data=val_dataset\n",
    "        )\n",
    "    history, elapsed_fitting_time = fit_model()\n",
    "    logging.info(f'Model fitting completed in {elapsed_fitting_time}')\n",
    "    mlflow.log_param('benchmark_model_fitting_time', elapsed_fitting_time)\n",
    "\n",
    "    logging.info(format_with_border('Evaluating Model on Test Data'))\n",
    "    test_datasets = create_test_datasets(\n",
    "        testing_stations.values(), FEATURE_COLUMNS, TARGET_COLUMN, SEQUENCE_LENGTH, TARGET_START_INDEX, DATASET_BATCH_SIZE, mean, std\n",
    "    )\n",
    "\n",
    "    all_evaluation_results = np.empty((0, 5), float)\n",
    "\n",
    "    for j, dataset in enumerate(test_datasets):\n",
    "        evaluation_results = model.evaluate(dataset, verbose=0)\n",
    "        precision = evaluation_results[2]\n",
    "        recall = evaluation_results[3]\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        station_name = list(testing_stations.keys())[j]\n",
    "        test_df = list(testing_stations.values())[j]\n",
    "        logging.info(\n",
    "            f'Station: {station_name}, Samples: {len(test_df)}, Loss: {evaluation_results[0]:.3f}, Accuracy: {evaluation_results[1]:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1_score:.3f}'\n",
    "        )\n",
    "        all_evaluation_results = np.append(all_evaluation_results, [evaluation_results + [f1_score]], axis=0)\n",
    "\n",
    "    mlflow.log_metric('test_avg_loss', np.mean(all_evaluation_results[:, 0]))\n",
    "    mlflow.log_metric('test_avg_accuracy', np.mean(all_evaluation_results[:, 1]))\n",
    "    mlflow.log_metric('test_avg_precision', np.mean(all_evaluation_results[:, 2]))\n",
    "    mlflow.log_metric('test_avg_recall', np.mean(all_evaluation_results[:, 3]))\n",
    "    mlflow.log_metric('test_avg_f1_score', np.mean(all_evaluation_results[:, 4]))\n",
    "\n",
    "    predictions = [model.predict(td, verbose=0).reshape((-1,)) > 0.5 for td in test_datasets]\n",
    "    fig = plot_data(\n",
    "        [test_station[TARGET_START_INDEX:] for test_station in testing_stations.values()],\n",
    "        predictions=predictions,\n",
    "        show=False\n",
    "    )\n",
    "    mlflow.log_figure(fig, 'prediction_results.png')\n",
    "    mlflow.log_artifact(tmp_log_file)\n",
    "    mlflow.log_figure(plot_keras_history(history), 'keras_history.png')\n",
    "    mlflow.log_figure(generate_confusion_matrix(), 'confusion_matrix.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
