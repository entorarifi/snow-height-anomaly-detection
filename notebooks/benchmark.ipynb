{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-13T14:25:08.822413267Z",
     "start_time": "2023-12-13T14:25:08.713935445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from dotenv import load_dotenv\n",
    "from keras.src.metrics import Recall, Precision\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from src.functions import create_model, plot_data, check_gpus, load_stations_from_path, \\\n",
    "    create_test_datasets, plot_keras_history, preprocces, mlflow_log_np_as_file, generate_confusion_matrix, \\\n",
    "    create_train_val_datasets\n",
    "from src.utils import now_formatted, setup_logger, format_with_border, measure_execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T14:25:09.983061235Z",
     "start_time": "2023-12-13T14:25:09.923958766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "check_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "mlflow_port = os.getenv('MLFLOW_PORT')\n",
    "mlflow_uri = f'http://localhost:{mlflow_port}'\n",
    "mlflow_experiment_name = f'Benchmark'\n",
    "train_path = '../data/labeled_benchmark/train'\n",
    "test_path = '../data/labeled_benchmark/test'\n",
    "log_file_path = '/tmp/benchmark.log'\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_uri)\n",
    "mlflow.set_experiment(mlflow_experiment_name)\n",
    "mlflow.tensorflow.autolog()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:25:12.107796481Z",
     "start_time": "2023-12-13T14:25:12.061765282Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 50\n",
    "TARGET_START_INDEX = SEQUENCE_LENGTH - 1\n",
    "FEATURE_COLUMNS = [\n",
    "    'HS',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'TSS_30MIN_MEAN',\n",
    "    'RSWR_30MIN_MEAN',\n",
    "    'TA_30MIN_MEAN',\n",
    "    'VW_30MIN_MEAN'\n",
    "]\n",
    "TARGET_COLUMN = 'no_snow'\n",
    "DATE_COLUMN = 'measure_date'\n",
    "SPLIT_PERCENTAGE = 0.8\n",
    "DATASET_BATCH_SIZE = 64\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ARCHITECTURE = \"128(l)-64-8(d)-1\"\n",
    "MODEL_INPUT_SHAPE = (SEQUENCE_LENGTH, len(FEATURE_COLUMNS))\n",
    "MODEL_DROPOUT_RATE = 0.5\n",
    "MODEL_OPTIMIZER = 'adam'\n",
    "MODEL_METRICS = ['accuracy', Recall(), Precision()],\n",
    "\n",
    "MODEL_LOSS = 'binary_crossentropy'\n",
    "MODEL_BATCH_SIZE = 64\n",
    "MODEL_EPOCHS = 15\n",
    "\n",
    "EXPERIMENT_NAME = 'TESTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT'\n",
    "\n",
    "def log_parameters(logging, mlflow):\n",
    "    global_vars = globals()\n",
    "    for var_name, value in global_vars.items():\n",
    "        if var_name.isupper():\n",
    "            logging.info(f'{var_name}: {value}')\n",
    "            mlflow.log_param(f'benchmark_{var_name.lower()}', value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:25:13.449621237Z",
     "start_time": "2023-12-13T14:25:13.387254936Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print model architecture\n",
    "import keras\n",
    "model = create_model(MODEL_ARCHITECTURE, MODEL_INPUT_SHAPE, logging=None, dropout_rate=0.5, summary=False, dropout_layer='normal')\n",
    "model.compile(\n",
    "    optimizer=MODEL_OPTIMIZER,\n",
    "    metrics=MODEL_METRICS,\n",
    "    loss=MODEL_LOSS\n",
    ")\n",
    "keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=False,\n",
    "    rankdir='TB',\n",
    "    expand_nested=True,\n",
    "    show_dtype=False,\n",
    "    # layer_range=True,\n",
    "    # show_layer_activations=True,\n",
    "    show_trainable=False,\n",
    "    to_file=\"model.png\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "training_stations = load_stations_from_path(train_path)\n",
    "testing_stations = {station.iloc[0]['station_code']: station for station in load_stations_from_path(test_path)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:25:19.654952401Z",
     "start_time": "2023-12-13T14:25:19.266792617Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mmlflow\u001B[49m\u001B[38;5;241m.\u001B[39mstart_run(run_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnow_formatted()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mEXPERIMENT_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m      2\u001B[0m     logging, tmp_log_file \u001B[38;5;241m=\u001B[39m setup_logger(log_file_path\u001B[38;5;241m=\u001B[39mlog_file_path)\n\u001B[1;32m      3\u001B[0m     logging\u001B[38;5;241m.\u001B[39minfo(format_with_border(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStarting experiment\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mlflow' is not defined"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=f\"{now_formatted()}_{EXPERIMENT_NAME}\"):\n",
    "    logging, tmp_log_file = setup_logger(log_file_path=log_file_path)\n",
    "    logging.info(format_with_border('Starting experiment'))\n",
    "    \n",
    "    log_parameters(logging, mlflow)\n",
    "    logging.info(format_with_border('Preparing Training Data'))\n",
    "    train_dataset, val_dataset, mean, std, num_train_samples, num_val_samples, _ = create_train_val_datasets(\n",
    "        training_stations, SPLIT_PERCENTAGE, FEATURE_COLUMNS, TARGET_COLUMN, SEQUENCE_LENGTH, TARGET_START_INDEX, DATASET_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    mlflow_log_np_as_file(mean, 'mean')\n",
    "    mlflow_log_np_as_file(std, 'std')\n",
    "\n",
    "    logging.info(f\"Training samples: {num_train_samples}\")\n",
    "    logging.info(f\"Validation samples: {num_val_samples}\")\n",
    "    mlflow.log_param('benchmark_training_samples', num_train_samples)\n",
    "    mlflow.log_param('benchmark_validation_samples', num_val_samples)\n",
    "\n",
    "    model = create_model(MODEL_ARCHITECTURE, MODEL_INPUT_SHAPE, logging=None, dropout_rate=MODEL_DROPOUT_RATE, summary=False, dropout_layer='dropout')\n",
    "    model.compile(\n",
    "        optimizer=MODEL_OPTIMIZER,\n",
    "        metrics=MODEL_METRICS,\n",
    "        loss=MODEL_LOSS\n",
    "    )\n",
    "    \n",
    "    logging.info(format_with_border('Fitting Model'))\n",
    "    @measure_execution_time\n",
    "    def fit_model():\n",
    "        return model.fit(\n",
    "            train_dataset,\n",
    "            epochs=MODEL_EPOCHS,\n",
    "            batch_size=MODEL_BATCH_SIZE,\n",
    "            validation_data=val_dataset\n",
    "        )\n",
    "    history, elapsed_fitting_time = fit_model()\n",
    "    logging.info(f'Model fitting completed in {elapsed_fitting_time}')\n",
    "    mlflow.log_param('benchmark_model_fitting_time', elapsed_fitting_time)\n",
    "\n",
    "    logging.info(format_with_border('Evaluating Model on Test Data'))\n",
    "    test_datasets = create_test_datasets(\n",
    "        testing_stations.values(), FEATURE_COLUMNS, TARGET_COLUMN, SEQUENCE_LENGTH, TARGET_START_INDEX, DATASET_BATCH_SIZE, mean, std\n",
    "    )\n",
    "\n",
    "    all_evaluation_results = np.empty((0, 5), float)\n",
    "\n",
    "    for j, dataset in enumerate(test_datasets):\n",
    "        evaluation_results = model.evaluate(dataset, verbose=0)\n",
    "        precision = evaluation_results[2]\n",
    "        recall = evaluation_results[3]\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        station_name = list(testing_stations.keys())[j]\n",
    "        test_df = list(testing_stations.values())[j]\n",
    "        logging.info(\n",
    "            f'Station: {station_name}, Samples: {len(test_df)}, Loss: {evaluation_results[0]:.3f}, Accuracy: {evaluation_results[1]:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1_score:.3f}'\n",
    "        )\n",
    "        all_evaluation_results = np.append(all_evaluation_results, [evaluation_results + [f1_score]], axis=0)\n",
    "\n",
    "    mlflow.log_metric('test_avg_loss', np.mean(all_evaluation_results[:, 0]))\n",
    "    mlflow.log_metric('test_avg_accuracy', np.mean(all_evaluation_results[:, 1]))\n",
    "    mlflow.log_metric('test_avg_precision', np.mean(all_evaluation_results[:, 2]))\n",
    "    mlflow.log_metric('test_avg_recall', np.mean(all_evaluation_results[:, 3]))\n",
    "    mlflow.log_metric('test_avg_f1_score', np.mean(all_evaluation_results[:, 4]))\n",
    "\n",
    "    predictions = [model.predict(td, verbose=0).reshape((-1,)) > 0.5 for td in test_datasets]\n",
    "    fig = plot_data(\n",
    "        [test_station[TARGET_START_INDEX:] for test_station in testing_stations.values()],\n",
    "        predictions=predictions,\n",
    "        show=False\n",
    "    )\n",
    "    mlflow.log_figure(fig, 'prediction_results.png')\n",
    "    mlflow.log_artifact(tmp_log_file)\n",
    "    mlflow.log_figure(plot_keras_history(history), 'keras_history.png')\n",
    "    mlflow.log_figure(generate_confusion_matrix(testing_stations, predictions, TARGET_START_INDEX), 'confusion_matrix.png')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T14:41:31.141942585Z",
     "start_time": "2023-12-13T14:41:30.628311639Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
