{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ebd1589ad7a1ac0",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-28T19:53:32.603692334Z",
     "start_time": "2023-10-28T19:53:31.487316938Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/labeled_daily'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [1], line 13\u001B[0m\n\u001B[1;32m      8\u001B[0m     files \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mlistdir(path)\n\u001B[1;32m     10\u001B[0m     pd\u001B[38;5;241m.\u001B[39mconcat(pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m files)\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmerged.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m \u001B[43mmerge_into_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [1], line 8\u001B[0m, in \u001B[0;36mmerge_into_one\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmerge_into_one\u001B[39m():\n\u001B[1;32m      7\u001B[0m     path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/labeled_daily\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 8\u001B[0m     files \u001B[38;5;241m=\u001B[39m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m     pd\u001B[38;5;241m.\u001B[39mconcat(pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m files)\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmerged.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/labeled_daily'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def merge_into_one():\n",
    "    path = '../data/labeled_daily'\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    pd.concat(pd.read_csv(f'{path}/{s}') for s in files).to_csv('merged.csv')\n",
    "\n",
    "\n",
    "merge_into_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def resample_and_save(read_path='../data/labeled', save_path='../data/labeled_daily'):\n",
    "    files = os.listdir(read_path)\n",
    "\n",
    "    for s in files:\n",
    "        station = pd.read_csv(f'{read_path}/{s}')\n",
    "        station['station_code'] = s[:-4]\n",
    "\n",
    "        station = preprocess(station, resample='D')\n",
    "\n",
    "        station.to_csv(f'{save_path}/{s}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2714879bb6cd3b1c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_stations(path, count, shuffle=False):\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(files)\n",
    "    else:\n",
    "        files = sorted(files)\n",
    "\n",
    "    dataframes = []\n",
    "    for s in files[:count]:\n",
    "        if not s.endswith('csv'):\n",
    "            continue\n",
    "        station = pd.read_csv(f'{path}/{s}')\n",
    "        station['station_code'] = s[:-4]\n",
    "        dataframes.append(station)\n",
    "\n",
    "    return pd.concat(dataframes, ignore_index=True), files[:count], files[count:]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa0ada527254b8cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_station(path, station_name, resample=None):\n",
    "    df = pd.read_csv(f'{path}/{station_name}')\n",
    "    df['station_code'] = station_name[:-4]\n",
    "\n",
    "    if resample is not None:\n",
    "        df = resample(df, resample)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "def preprocess(df, scale=False, resample=None):\n",
    "    data = df[['station_code', 'measure_date', 'HS', 'no_snow', 'anomaly']].copy()\n",
    "    data['measure_date'] = pd.to_datetime(data['measure_date']).dt.tz_localize(None)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    if scale:\n",
    "        # def scale_group(group):\n",
    "        #     scaler = StandardScaler()\n",
    "        #     # TODO: Could try standardizing and then scaling\n",
    "        #     group['HS'] = scaler.fit_transform(group[['HS']])\n",
    "        #     return group\n",
    "\n",
    "        # data = data.groupby('station_code').apply(scale_group).reset_index(drop=True)\n",
    "        data['HS'] = MinMaxScaler().fit_transform(data[['HS']])\n",
    "\n",
    "    if resample:\n",
    "        data.set_index('measure_date', inplace=True)\n",
    "        data = data.groupby('station_code').resample(resample).agg({\n",
    "            'HS': lambda x: x.iloc[np.argmax(np.abs(x.values - x.mean()))],\n",
    "            'no_snow': lambda x: x.value_counts().idxmax(), # TODO: Could try x.any() to improve predictions?\n",
    "            'anomaly': lambda x: x.any()\n",
    "        }).reset_index()\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e243c98a8f2dc47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "from datetime import datetime\n",
    "\n",
    "SEQ_SIZE = 20\n",
    "EPOCHS = 50\n",
    "TRAIN_SIZE = 3\n",
    "VALIDATION_PERCENTAGE = 0.3\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_SEQS = False\n",
    "SCALE = True\n",
    "\n",
    "current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_name = f\"{current_time}_epochs-{EPOCHS}_seq-{SEQ_SIZE}_train-{TRAIN_SIZE}_test-{VALIDATION_PERCENTAGE}_shuffle-seqs-{SHUFFLE_SEQS}_scale-{SCALE}\"\n",
    "log_dir = f\"../logs/{model_name}\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e86723a60ce0d59e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
